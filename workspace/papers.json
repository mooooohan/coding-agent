[
  {
    "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
    "published": "2025-12-11T18:59:56Z",
    "updated": "2025-12-11T18:59:56Z",
    "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
    "link": "http://arxiv.org/abs/2512.10957v1",
    "authors": [
      "Yukai Shi",
      "Weiyu Li",
      "Zihao Wang",
      "Hongyang Li",
      "Xingyu Chen",
      "Ping Tan",
      "Lei Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
    "published": "2025-12-11T18:59:55Z",
    "updated": "2025-12-11T18:59:55Z",
    "summary": "The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.",
    "link": "http://arxiv.org/abs/2512.10952v1",
    "authors": [
      "Xiaona Zhou",
      "Yingyan Zeng",
      "Ran Jin",
      "Ismini Lourentzou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "published": "2025-12-11T18:59:52Z",
    "updated": "2025-12-11T18:59:52Z",
    "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
    "link": "http://arxiv.org/abs/2512.10949v1",
    "authors": [
      "Yiwen Tang",
      "Zoey Guo",
      "Kaixin Zhu",
      "Ray Zhang",
      "Qizhi Chen",
      "Dongzhi Jiang",
      "Junli Liu",
      "Bohan Zeng",
      "Haoming Song",
      "Delin Qu",
      "Tianyi Bai",
      "Dan Xu",
      "Wentao Zhang",
      "Bin Zhao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "title": "ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning",
    "published": "2025-12-11T18:59:46Z",
    "updated": "2025-12-11T18:59:46Z",
    "summary": "Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.",
    "link": "http://arxiv.org/abs/2512.10946v1",
    "authors": [
      "Wendi Chen",
      "Han Xue",
      "Yi Wang",
      "Fangyuan Zhou",
      "Jun Lv",
      "Yang Jin",
      "Shirun Tang",
      "Chuan Wen",
      "Cewu Lu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "title": "AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation",
    "published": "2025-12-11T18:59:34Z",
    "updated": "2025-12-11T18:59:34Z",
    "summary": "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT",
    "link": "http://arxiv.org/abs/2512.10943v1",
    "authors": [
      "Sharath Girish",
      "Viacheslav Ivanov",
      "Tsai-Shien Chen",
      "Hao Chen",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "title": "Mull-Tokens: Modality-Agnostic Latent Thinking",
    "published": "2025-12-11T18:59:08Z",
    "updated": "2025-12-11T18:59:08Z",
    "summary": "Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.",
    "link": "http://arxiv.org/abs/2512.10941v1",
    "authors": [
      "Arijit Ray",
      "Ahmed Abdelkader",
      "Chengzhi Mao",
      "Bryan A. Plummer",
      "Kate Saenko",
      "Ranjay Krishna",
      "Leonidas Guibas",
      "Wen-Sheng Chu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
    "published": "2025-12-11T18:59:05Z",
    "updated": "2025-12-11T18:59:05Z",
    "summary": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
    "link": "http://arxiv.org/abs/2512.10940v1",
    "authors": [
      "Xiang Fan",
      "Sharath Girish",
      "Vivek Ramanujan",
      "Chaoyang Wang",
      "Ashkan Mirzaei",
      "Petr Sushko",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov",
      "Ranjay Krishna"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "published": "2025-12-11T18:58:49Z",
    "updated": "2025-12-11T18:58:49Z",
    "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\\mathrm{Derf}(x) = \\mathrm{erf}(Î±x + s)$, where $\\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
    "link": "http://arxiv.org/abs/2512.10938v1",
    "authors": [
      "Mingzhi Chen",
      "Taiming Lu",
      "Jiachen Zhu",
      "Mingjie Sun",
      "Zhuang Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "title": "On Decision-Making Agents and Higher-Order Causal Processes",
    "published": "2025-12-11T18:58:33Z",
    "updated": "2025-12-11T18:58:33Z",
    "summary": "We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.",
    "link": "http://arxiv.org/abs/2512.10937v1",
    "authors": [
      "Matt Wilson"
    ],
    "categories": [
      "cs.AI",
      "quant-ph"
    ]
  },
  {
    "title": "Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks",
    "published": "2025-12-11T18:58:17Z",
    "updated": "2025-12-11T18:58:17Z",
    "summary": "The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).",
    "link": "http://arxiv.org/abs/2512.10936v1",
    "authors": [
      "Kristina Korotkova",
      "Aleksandr Katrutsa"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "published": "2025-12-11T18:59:59Z",
    "updated": "2025-12-11T18:59:59Z",
    "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
    "link": "http://arxiv.org/abs/2512.10959v1",
    "authors": [
      "Tjark Behrens",
      "Anton Obukhov",
      "Bingxin Ke",
      "Fabio Tosi",
      "Matteo Poggi",
      "Konrad Schindler"
    ],
    "categories": [
      "cs.CV"
    ]
  },
  {
    "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
    "published": "2025-12-11T18:59:58Z",
    "updated": "2025-12-11T18:59:58Z",
    "summary": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
    "link": "http://arxiv.org/abs/2512.10958v1",
    "authors": [
      "Ao Liang",
      "Lingdong Kong",
      "Tianyi Yan",
      "Hongsi Liu",
      "Wesley Yang",
      "Ziqi Huang",
      "Wei Yin",
      "Jialong Zuo",
      "Yixuan Hu",
      "Dekai Zhu",
      "Dongyue Lu",
      "Youquan Liu",
      "Guangfeng Jiang",
      "Linfeng Li",
      "Xiangtai Li",
      "Long Zhuo",
      "Lai Xing Ng",
      "Benoit R. Cottereau",
      "Changxin Gao",
      "Liang Pan",
      "Wei Tsang Ooi",
      "Ziwei Liu"
    ],
    "categories": [
      "cs.CV"
    ]
  },
  {
    "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
    "published": "2025-12-11T18:59:56Z",
    "updated": "2025-12-11T18:59:56Z",
    "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
    "link": "http://arxiv.org/abs/2512.10957v1",
    "authors": [
      "Yukai Shi",
      "Weiyu Li",
      "Zihao Wang",
      "Hongyang Li",
      "Xingyu Chen",
      "Ping Tan",
      "Lei Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
    "published": "2025-12-11T18:59:56Z",
    "updated": "2025-12-11T18:59:56Z",
    "summary": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
    "link": "http://arxiv.org/abs/2512.10956v1",
    "authors": [
      "Wentao Zhou",
      "Xuweiyi Chen",
      "Vignesh Rajagopal",
      "Jeffrey Chen",
      "Rohan Chandra",
      "Zezhou Cheng"
    ],
    "categories": [
      "cs.CV"
    ]
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "published": "2025-12-11T18:59:56Z",
    "updated": "2025-12-11T18:59:56Z",
    "summary": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
    "link": "http://arxiv.org/abs/2512.10955v1",
    "authors": [
      "Tsai-Shien Chen",
      "Aliaksandr Siarohin",
      "Guocheng Gordon Qian",
      "Kuan-Chieh Jackson Wang",
      "Egor Nemchinov",
      "Moayed Haji-Ali",
      "Riza Alp Guler",
      "Willi Menapace",
      "Ivan Skorokhodov",
      "Anil Kag",
      "Jun-Yan Zhu",
      "Sergey Tulyakov"
    ],
    "categories": [
      "cs.CV"
    ]
  },
  {
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "published": "2025-12-11T18:59:55Z",
    "updated": "2025-12-11T18:59:55Z",
    "summary": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.",
    "link": "http://arxiv.org/abs/2512.10953v1",
    "authors": [
      "Yiyang Lu",
      "Qiao Sun",
      "Xianbang Wang",
      "Zhicheng Jiang",
      "Hanhong Zhao",
      "Kaiming He"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "title": "Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration",
    "published": "2025-12-11T18:59:55Z",
    "updated": "2025-12-11T18:59:55Z",
    "summary": "In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.",
    "link": "http://arxiv.org/abs/2512.10954v1",
    "authors": [
      "Sicheng Mo",
      "Thao Nguyen",
      "Richard Zhang",
      "Nick Kolkin",
      "Siddharth Srinivasan Iyer",
      "Eli Shechtman",
      "Krishna Kumar Singh",
      "Yong Jae Lee",
      "Bolei Zhou",
      "Yuheng Li"
    ],
    "categories": [
      "cs.CV"
    ]
  },
  {
    "title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training",
    "published": "2025-12-11T18:59:53Z",
    "updated": "2025-12-11T18:59:53Z",
    "summary": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.",
    "link": "http://arxiv.org/abs/2512.10950v1",
    "authors": [
      "Qitao Zhao",
      "Hao Tan",
      "Qianqian Wang",
      "Sai Bi",
      "Kai Zhang",
      "Kalyan Sunkavalli",
      "Shubham Tulsiani",
      "Hanwen Jiang"
    ],
    "categories": [
      "cs.CV"
    ]
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "published": "2025-12-11T18:59:52Z",
    "updated": "2025-12-11T18:59:52Z",
    "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
    "link": "http://arxiv.org/abs/2512.10949v1",
    "authors": [
      "Yiwen Tang",
      "Zoey Guo",
      "Kaixin Zhu",
      "Ray Zhang",
      "Qizhi Chen",
      "Dongzhi Jiang",
      "Junli Liu",
      "Bohan Zeng",
      "Haoming Song",
      "Delin Qu",
      "Tianyi Bai",
      "Dan Xu",
      "Wentao Zhang",
      "Bin Zhao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "title": "ClusIR: Towards Cluster-Guided All-in-One Image Restoration",
    "published": "2025-12-11T18:59:47Z",
    "updated": "2025-12-11T18:59:47Z",
    "summary": "All-in-One Image Restoration (AiOIR) aims to recover high-quality images from diverse degradations within a unified framework. However, existing methods often fail to explicitly model degradation types and struggle to adapt their restoration behavior to complex or mixed degradations. To address these issues, we propose ClusIR, a Cluster-Guided Image Restoration framework that explicitly models degradation semantics through learnable clustering and propagates cluster-aware cues across spatial and frequency domains for adaptive restoration. Specifically, ClusIR comprises two key components: a Probabilistic Cluster-Guided Routing Mechanism (PCGRM) and a Degradation-Aware Frequency Modulation Module (DAFMM). The proposed PCGRM disentangles degradation recognition from expert activation, enabling discriminative degradation perception and stable expert routing. Meanwhile, DAFMM leverages the cluster-guided priors to perform adaptive frequency decomposition and targeted modulation, collaboratively refining structural and textural representations for higher restoration fidelity. The cluster-guided synergy seamlessly bridges semantic cues with frequency-domain modulation, empowering ClusIR to attain remarkable restoration results across a wide range of degradations. Extensive experiments on diverse benchmarks validate that ClusIR reaches competitive performance under several scenarios.",
    "link": "http://arxiv.org/abs/2512.10948v1",
    "authors": [
      "Shengkai Hu",
      "Jiaqi Ma",
      "Jun Wan",
      "Wenwen Min",
      "Yongcheng Jing",
      "Lefei Zhang",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.CV"
    ]
  },
  {
    "title": "Zorya: Automated Concolic Execution of Single-Threaded Go Binaries",
    "published": "2025-12-11T16:43:51Z",
    "updated": "2025-12-11T16:43:51Z",
    "summary": "Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.",
    "link": "http://arxiv.org/abs/2512.10799v1",
    "authors": [
      "Karolina Gorna",
      "Nicolas Iooss",
      "Yannick Seurin",
      "Rida Khatoun"
    ],
    "categories": [
      "cs.SE",
      "cs.PL"
    ]
  },
  {
    "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
    "published": "2025-12-11T14:49:56Z",
    "updated": "2025-12-11T14:49:56Z",
    "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
    "link": "http://arxiv.org/abs/2512.10713v1",
    "authors": [
      "Itay Dreyfuss",
      "Antonio Abu Nassar",
      "Samuel Ackerman",
      "Axel Ben David",
      "Rami Katan",
      "Orna Raz",
      "Marcel Zalmanovici"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "title": "Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories",
    "published": "2025-12-11T13:16:20Z",
    "updated": "2025-12-11T13:16:20Z",
    "summary": "Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.",
    "link": "http://arxiv.org/abs/2512.10618v1",
    "authors": [
      "Georgia M. Kapitsaki",
      "Maria Papoutsoglou",
      "Christoph Treude",
      "Ioanna Theophilou"
    ],
    "categories": [
      "cs.SE"
    ]
  },
  {
    "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild",
    "published": "2025-12-11T10:14:42Z",
    "updated": "2025-12-11T10:14:42Z",
    "summary": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.",
    "link": "http://arxiv.org/abs/2512.10493v1",
    "authors": [
      "Binquan Zhang",
      "Li Zhang",
      "Haoyuan Zhang",
      "Fang Liu",
      "Song Wang",
      "Bo Shen",
      "An Fu",
      "Lin Shi"
    ],
    "categories": [
      "cs.SE"
    ]
  },
  {
    "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection",
    "published": "2025-12-11T10:04:54Z",
    "updated": "2025-12-11T10:04:54Z",
    "summary": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.",
    "link": "http://arxiv.org/abs/2512.10485v1",
    "authors": [
      "Chaomeng Lu",
      "Bert Lagaisse"
    ],
    "categories": [
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ]
  },
  {
    "title": "UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval",
    "published": "2025-12-11T09:15:38Z",
    "updated": "2025-12-11T09:15:38Z",
    "summary": "Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.",
    "link": "http://arxiv.org/abs/2512.10452v1",
    "authors": [
      "Yang Yang",
      "Li Kuang",
      "Jiakun Liu",
      "Zhongxin Liu",
      "Yingjie Xia",
      "David Lo"
    ],
    "categories": [
      "cs.SE"
    ]
  },
  {
    "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation",
    "published": "2025-12-11T08:28:33Z",
    "updated": "2025-12-11T08:28:33Z",
    "summary": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.",
    "link": "http://arxiv.org/abs/2512.10415v1",
    "authors": [
      "Devanshu Sahoo",
      "Vasudev Majhi",
      "Arjun Neekhra",
      "Yash Sinha",
      "Murari Mandal",
      "Dhruv Kumar"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "published": "2025-12-11T08:05:58Z",
    "updated": "2025-12-11T08:05:58Z",
    "summary": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
    "link": "http://arxiv.org/abs/2512.10398v1",
    "authors": [
      "Zhaodong Wang",
      "Zhenting Qi",
      "Sherman Wong",
      "Nathan Hu",
      "Samuel Lin",
      "Jun Ge",
      "Erwin Gao",
      "Yining Yang",
      "Ben Maurer",
      "Wenlin Chen",
      "David Recordon",
      "Yilun Du",
      "Minlan Yu",
      "Ying Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ]
  },
  {
    "title": "Cross-modal Retrieval Models for Stripped Binary Analysis",
    "published": "2025-12-11T07:58:10Z",
    "updated": "2025-12-11T07:58:10Z",
    "summary": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.",
    "link": "http://arxiv.org/abs/2512.10393v1",
    "authors": [
      "Guoqiang Chen",
      "Lingyun Ying",
      "Ziyang Song",
      "Daguang Liu",
      "Qiang Wang",
      "Zhiqi Wang",
      "Li Hu",
      "Shaoyin Cheng",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "title": "Studying and Automating Issue Resolution for Software Quality",
    "published": "2025-12-11T02:44:40Z",
    "updated": "2025-12-11T02:44:40Z",
    "summary": "Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.",
    "link": "http://arxiv.org/abs/2512.10238v1",
    "authors": [
      "Antu Saha"
    ],
    "categories": [
      "cs.SE"
    ]
  },
  {
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "published": "2025-12-11T18:59:55Z",
    "updated": "2025-12-11T18:59:55Z",
    "summary": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.",
    "link": "http://arxiv.org/abs/2512.10953v1",
    "authors": [
      "Yiyang Lu",
      "Qiao Sun",
      "Xianbang Wang",
      "Zhicheng Jiang",
      "Hanhong Zhao",
      "Kaiming He"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
    "published": "2025-12-11T18:59:55Z",
    "updated": "2025-12-11T18:59:55Z",
    "summary": "The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.",
    "link": "http://arxiv.org/abs/2512.10952v1",
    "authors": [
      "Xiaona Zhou",
      "Yingyan Zeng",
      "Ran Jin",
      "Ismini Lourentzou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "title": "ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning",
    "published": "2025-12-11T18:59:46Z",
    "updated": "2025-12-11T18:59:46Z",
    "summary": "Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.",
    "link": "http://arxiv.org/abs/2512.10946v1",
    "authors": [
      "Wendi Chen",
      "Han Xue",
      "Yi Wang",
      "Fangyuan Zhou",
      "Jun Lv",
      "Yang Jin",
      "Shirun Tang",
      "Chuan Wen",
      "Cewu Lu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "published": "2025-12-11T18:58:49Z",
    "updated": "2025-12-11T18:58:49Z",
    "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\\mathrm{Derf}(x) = \\mathrm{erf}(Î±x + s)$, where $\\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
    "link": "http://arxiv.org/abs/2512.10938v1",
    "authors": [
      "Mingzhi Chen",
      "Taiming Lu",
      "Jiachen Zhu",
      "Mingjie Sun",
      "Zhuang Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "title": "Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks",
    "published": "2025-12-11T18:58:17Z",
    "updated": "2025-12-11T18:58:17Z",
    "summary": "The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).",
    "link": "http://arxiv.org/abs/2512.10936v1",
    "authors": [
      "Kristina Korotkova",
      "Aleksandr Katrutsa"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "title": "Any4D: Unified Feed-Forward Metric 4D Reconstruction",
    "published": "2025-12-11T18:57:39Z",
    "updated": "2025-12-11T18:57:39Z",
    "summary": "We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.",
    "link": "http://arxiv.org/abs/2512.10935v1",
    "authors": [
      "Jay Karhade",
      "Nikhil Keetha",
      "Yuchen Zhang",
      "Tanisha Gupta",
      "Akash Sharma",
      "Sebastian Scherer",
      "Deva Ramanan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ]
  },
  {
    "title": "Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit",
    "published": "2025-12-11T18:57:29Z",
    "updated": "2025-12-11T18:57:29Z",
    "summary": "Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.   The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.",
    "link": "http://arxiv.org/abs/2512.10934v1",
    "authors": [
      "Zamirddine Mari",
      "JÃ©rÃ´me Pasquet",
      "Julien Seinturier"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ]
  },
  {
    "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs",
    "published": "2025-12-11T18:57:02Z",
    "updated": "2025-12-11T18:57:02Z",
    "summary": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",
    "link": "http://arxiv.org/abs/2512.10931v1",
    "authors": [
      "George Yakushev",
      "Nataliia Babina",
      "Masoud Vahid Dastgerdi",
      "Vyacheslav Zhdanovskiy",
      "Alina Shutova",
      "Denis Kuznedelev"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "title": "Noisy Quantum Learning Theory",
    "published": "2025-12-11T18:56:32Z",
    "updated": "2025-12-11T18:56:32Z",
    "summary": "We develop a framework for learning from noisy quantum experiments, focusing on fault-tolerant devices accessing uncharacterized systems through noisy couplings. Our starting point is the complexity class $\\textsf{NBQP}$ (\"noisy BQP\"), modeling noisy fault-tolerant quantum computers that cannot, in general, error-correct the oracle systems they query. Using this class, we show that for natural oracle problems, noise can eliminate exponential quantum learning advantages of ideal noiseless learners while preserving a superpolynomial gap between NISQ and fault-tolerant devices. Beyond oracle separations, we study concrete noisy learning tasks. For purity testing, the exponential two-copy advantage collapses under a single application of local depolarizing noise. Nevertheless, we identify a setting motivated by AdS/CFT in which noise-resilient structure restores a quantum learning advantage in a noisy regime. We then analyze noisy Pauli shadow tomography, deriving lower bounds that characterize how instance size, quantum memory, and noise control sample complexity, and design algorithms with parametrically similar scalings. Together, our results show that the Bell-basis and SWAP-test primitives underlying most exponential quantum learning advantages are fundamentally fragile to noise unless the experimental system has latent noise-robust structure. Thus, realizing meaningful quantum advantages in future experiments will require understanding how noise-robust physical properties interface with available algorithmic techniques.",
    "link": "http://arxiv.org/abs/2512.10929v1",
    "authors": [
      "Jordan Cotler",
      "Weiyuan Gong",
      "Ishaan Kannan"
    ],
    "categories": [
      "quant-ph",
      "cs.CC",
      "cs.IT",
      "cs.LG"
    ]
  },
  {
    "title": "Decoupled Q-Chunking",
    "published": "2025-12-11T18:52:51Z",
    "updated": "2025-12-11T18:52:51Z",
    "summary": "Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences (\"chunks\") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.",
    "link": "http://arxiv.org/abs/2512.10926v1",
    "authors": [
      "Qiyang Li",
      "Seohong Park",
      "Sergey Levine"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ]
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "published": "2025-12-11T18:59:52Z",
    "updated": "2025-12-11T18:59:52Z",
    "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
    "link": "http://arxiv.org/abs/2512.10949v1",
    "authors": [
      "Yiwen Tang",
      "Zoey Guo",
      "Kaixin Zhu",
      "Ray Zhang",
      "Qizhi Chen",
      "Dongzhi Jiang",
      "Junli Liu",
      "Bohan Zeng",
      "Haoming Song",
      "Delin Qu",
      "Tianyi Bai",
      "Dan Xu",
      "Wentao Zhang",
      "Bin Zhao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "published": "2025-12-11T18:58:49Z",
    "updated": "2025-12-11T18:58:49Z",
    "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\\mathrm{Derf}(x) = \\mathrm{erf}(Î±x + s)$, where $\\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
    "link": "http://arxiv.org/abs/2512.10938v1",
    "authors": [
      "Mingzhi Chen",
      "Taiming Lu",
      "Jiachen Zhu",
      "Mingjie Sun",
      "Zhuang Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs",
    "published": "2025-12-11T18:57:02Z",
    "updated": "2025-12-11T18:57:02Z",
    "summary": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",
    "link": "http://arxiv.org/abs/2512.10931v1",
    "authors": [
      "George Yakushev",
      "Nataliia Babina",
      "Masoud Vahid Dastgerdi",
      "Vyacheslav Zhdanovskiy",
      "Alina Shutova",
      "Denis Kuznedelev"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "title": "CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio for Social Co-Viewing Experiences",
    "published": "2025-12-11T18:44:44Z",
    "updated": "2025-12-11T18:44:44Z",
    "summary": "Social presence is central to the enjoyment of watching content together, yet modern media consumption is increasingly solitary. We investigate whether multi-agent conversational AI systems can recreate the dynamics of shared viewing experiences across diverse content types. We present CompanionCast, a general framework for orchestrating multiple role-specialized AI agents that respond to video content using multimodal inputs, speech synthesis, and spatial audio. Distinctly, CompanionCast integrates an LLM-as-a-Judge module that iteratively scores and refines conversations across five dimensions (relevance, authenticity, engagement, diversity, personality consistency). We validate this framework through sports viewing, a domain with rich dynamics and strong social traditions, where a pilot study with soccer fans suggests that multi-agent interaction improves perceived social presence compared to solo viewing. We contribute: (1) a generalizable framework for orchestrating multi-agent conversations around multimodal video content, (2) a novel evaluator-agent pipeline for conversation quality control, and (3) exploratory evidence of increased social presence in AI-mediated co-viewing. We discuss challenges and future directions for applying this approach to diverse viewing contexts including entertainment, education, and collaborative watching experiences.",
    "link": "http://arxiv.org/abs/2512.10918v1",
    "authors": [
      "Yiyang Wang",
      "Chen Chen",
      "Tica Lin",
      "Vishnu Raj",
      "Josh Kimball",
      "Alex Cabral",
      "Josiah Hester"
    ],
    "categories": [
      "cs.HC",
      "cs.CL"
    ]
  },
  {
    "title": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity",
    "published": "2025-12-11T18:11:46Z",
    "updated": "2025-12-11T18:11:46Z",
    "summary": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.",
    "link": "http://arxiv.org/abs/2512.10882v1",
    "authors": [
      "Hauke Licht"
    ],
    "categories": [
      "cs.CL"
    ]
  },
  {
    "title": "Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python",
    "published": "2025-12-11T17:58:17Z",
    "updated": "2025-12-11T17:58:17Z",
    "summary": "This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.",
    "link": "http://arxiv.org/abs/2512.10865v1",
    "authors": [
      "Lilin Qiu"
    ],
    "categories": [
      "cs.CL"
    ]
  },
  {
    "title": "LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification",
    "published": "2025-12-11T16:39:07Z",
    "updated": "2025-12-11T16:39:07Z",
    "summary": "LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.",
    "link": "http://arxiv.org/abs/2512.10793v1",
    "authors": [
      "Michael Schlee",
      "Christoph Weisser",
      "Timo KivimÃ¤ki",
      "Melchizedek Mashiku",
      "Benjamin Saefken"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "published": "2025-12-11T16:35:14Z",
    "updated": "2025-12-11T16:35:14Z",
    "summary": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
    "link": "http://arxiv.org/abs/2512.10791v1",
    "authors": [
      "Aileen Cheng",
      "Alon Jacovi",
      "Amir Globerson",
      "Ben Golan",
      "Charles Kwong",
      "Chris Alberti",
      "Connie Tao",
      "Eyal Ben-David",
      "Gaurav Singh Tomar",
      "Lukas Haas",
      "Yonatan Bitton",
      "Adam Bloniarz",
      "Aijun Bai",
      "Andrew Wang",
      "Anfal Siddiqui",
      "Arturo Bajuelos Castillo",
      "Aviel Atias",
      "Chang Liu",
      "Corey Fry",
      "Daniel Balle",
      "Deepanway Ghosal",
      "Doron Kukliansky",
      "Dror Marcus",
      "Elena Gribovskaya",
      "Eran Ofek",
      "Honglei Zhuang",
      "Itay Laish",
      "Jan Ackermann",
      "Lily Wang",
      "Meg Risdal",
      "Megan Barnes",
      "Michael Fink",
      "Mohamed Amin",
      "Moran Ambar",
      "Natan Potikha",
      "Nikita Gupta",
      "Nitzan Katz",
      "Noam Velan",
      "Ofir Roval",
      "Ori Ram",
      "Polina Zablotskaia",
      "Prathamesh Bang",
      "Priyanka Agrawal",
      "Rakesh Ghiya",
      "Sanjay Ganapathy",
      "Simon Baumgartner",
      "Sofia Erell",
      "Sushant Prakash",
      "Thibault Sellam",
      "Vikram Rao",
      "Xuanhui Wang",
      "Yaroslav Akulov",
      "Yulong Yang",
      "Zhen Yang",
      "Zhixin Lai",
      "Zhongru Wu",
      "Anca Dragan",
      "Avinatan Hassidim",
      "Fernando Pereira",
      "Slav Petrov",
      "Srinivasan Venkatachary",
      "Tulsee Doshi",
      "Yossi Matias",
      "Sasha Goldshtein",
      "Dipanjan Das"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly",
    "published": "2025-12-11T16:31:29Z",
    "updated": "2025-12-11T16:31:29Z",
    "summary": "Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \\textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \\textbf{context dilution}, where distractors crowd out relevant information. We propose \\textbf{SEAL-RAG}, a training-free controller that adopts a \\textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\\textbf{S}earch $\\rightarrow$ \\textbf{E}xtract $\\rightarrow$ \\textbf{A}ssess $\\rightarrow$ \\textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \\textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \\textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \\textbf{HotpotQA} and \\textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \\textbf{+3--13 pp} and evidence precision by \\textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \\textbf{+8.0 pp} in accuracy and maintains \\textbf{96\\%} evidence precision compared to 22\\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.",
    "link": "http://arxiv.org/abs/2512.10787v1",
    "authors": [
      "Moshe Lahmy",
      "Roi Yozevitch"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "title": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting",
    "published": "2025-12-11T16:15:42Z",
    "updated": "2025-12-11T16:15:42Z",
    "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.",
    "link": "http://arxiv.org/abs/2512.10780v1",
    "authors": [
      "Manurag Khullar",
      "Utkarsh Desai",
      "Poorva Malviya",
      "Aman Dalmia",
      "Zheyuan Ryan Shi"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "title": "A Spiking Neural Network Implementation of Gaussian Belief Propagation",
    "published": "2025-12-11T13:43:42Z",
    "updated": "2025-12-11T13:43:42Z",
    "summary": "Bayesian inference offers a principled account of information processing in natural agents. However, it remains an open question how neural mechanisms perform their abstract operations. We investigate a hypothesis where a distributed form of Bayesian inference, namely message passing on factor graphs, is performed by a simulated network of leaky-integrate-and-fire neurons. Specifically, we perform Gaussian belief propagation by encoding messages that come into factor nodes as spike-based signals, propagating these signals through a spiking neural network (SNN) and decoding the spike-based signal back to an outgoing message. Three core linear operations, equality (branching), addition, and multiplication, are realized in networks of leaky integrate-and-fire models. Validation against the standard sum-product algorithm shows accurate message updates, while applications to Kalman filtering and Bayesian linear regression demonstrate the framework's potential for both static and dynamic inference tasks. Our results provide a step toward biologically grounded, neuromorphic implementations of probabilistic reasoning.",
    "link": "http://arxiv.org/abs/2512.10638v1",
    "authors": [
      "Sepideh Adamiat",
      "Wouter M. Kouw",
      "Bert de Vries"
    ],
    "categories": [
      "cs.NE"
    ]
  },
  {
    "title": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs",
    "published": "2025-12-11T13:04:44Z",
    "updated": "2025-12-11T13:04:44Z",
    "summary": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.",
    "link": "http://arxiv.org/abs/2512.10611v1",
    "authors": [
      "Minghao LI",
      "Ruihang Wang",
      "Rui Tan",
      "Yonggang Wen"
    ],
    "categories": [
      "cs.AI",
      "cs.NE"
    ]
  },
  {
    "title": "Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots",
    "published": "2025-12-11T09:55:49Z",
    "updated": "2025-12-11T09:55:49Z",
    "summary": "In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. \"Swaddling\" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.",
    "link": "http://arxiv.org/abs/2512.10477v1",
    "authors": [
      "Timur Ishuov",
      "Michele Folgheraiter",
      "Madi Nurmanov",
      "Goncalo Gordo",
      "RichÃ¡rd Farkas",
      "JÃ³zsef Dombi"
    ],
    "categories": [
      "cs.RO",
      "cs.NE"
    ]
  },
  {
    "title": "An exploration for higher efficiency in multi objective optimisation with reinforcement learning",
    "published": "2025-12-11T01:58:04Z",
    "updated": "2025-12-11T01:58:04Z",
    "summary": "Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.",
    "link": "http://arxiv.org/abs/2512.10208v1",
    "authors": [
      "Mehmet Emin Aydin"
    ],
    "categories": [
      "cs.AI",
      "cs.NE"
    ]
  },
  {
    "title": "Spatial Spiking Neural Networks Enable Efficient and Robust Temporal Computation",
    "published": "2025-12-10T19:01:39Z",
    "updated": "2025-12-10T19:01:39Z",
    "summary": "The efficiency of modern machine intelligence depends on high accuracy with minimal computational cost. In spiking neural networks (SNNs), synaptic delays are crucial for encoding temporal structure, yet existing models treat them as fully trainable, unconstrained parameters, leading to large memory footprints, higher computational demand, and a departure from biological plausibility. In the brain, however, delays arise from physical distances between neurons embedded in space. Building on this principle, we introduce Spatial Spiking Neural Networks (SpSNNs), a framework in which neurons learn coordinates in a finite-dimensional Euclidean space and delays emerge from inter-neuron distances. This replaces per-synapse delay learning with position learning, substantially reducing parameter count while retaining temporal expressiveness. Across the Yin-Yang and Spiking Heidelberg Digits benchmarks, SpSNNs outperform SNNs with unconstrained delays despite using far fewer parameters. Performance consistently peaks in 2D and 3D networks rather than infinite-dimensional delay spaces, revealing a geometric regularization effect. Moreover, dynamically sparsified SpSNNs maintain full accuracy even at 90% sparsity, matching standard delay-trained SNNs while using up to 18x fewer parameters. Because learned spatial layouts map naturally onto hardware geometries, SpSNNs lend themselves to efficient neuromorphic implementation. Methodologically, SpSNNs compute exact delay gradients via automatic differentiation with custom-derived rules, supporting arbitrary neuron models and architectures. Altogether, SpSNNs provide a principled platform for exploring spatial structure in temporal computation and offer a hardware-friendly substrate for scalable, energy-efficient neuromorphic intelligence.",
    "link": "http://arxiv.org/abs/2512.10011v1",
    "authors": [
      "Lennart P. L. Landsmeer",
      "Amirreza Movahedin",
      "Mario Negrello",
      "Said Hamdioui",
      "Christos Strydis"
    ],
    "categories": [
      "cs.NE",
      "q-bio.NC"
    ]
  },
  {
    "title": "BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization",
    "published": "2025-12-10T15:32:56Z",
    "updated": "2025-12-10T15:32:56Z",
    "summary": "Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the \"curse of dimensionality,\" rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.",
    "link": "http://arxiv.org/abs/2512.09972v1",
    "authors": [
      "Kesheng Chen",
      "Wenjian Luo",
      "Zhenqian Zhu",
      "Yamin Hu",
      "Yiya Xi"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.NE"
    ]
  },
  {
    "title": "Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power",
    "published": "2025-12-10T14:18:59Z",
    "updated": "2025-12-10T14:18:59Z",
    "summary": "Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.",
    "link": "http://arxiv.org/abs/2512.09673v1",
    "authors": [
      "Yuzhu Chen",
      "Tian Qin",
      "Xinmei Tian",
      "Fengxiang He",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ]
  },
  {
    "title": "Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates",
    "published": "2025-12-10T12:23:04Z",
    "updated": "2025-12-10T12:23:04Z",
    "summary": "Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery.",
    "link": "http://arxiv.org/abs/2512.09586v1",
    "authors": [
      "Prashant Kumar Choudhary",
      "Nouhaila Innan",
      "Muhammad Shafique",
      "Rajeev Singh"
    ],
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.NI"
    ]
  },
  {
    "title": "Neuromorphic Eye Tracking for Low-Latency Pupil Detection",
    "published": "2025-12-10T11:30:21Z",
    "updated": "2025-12-10T11:30:21Z",
    "summary": "Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.",
    "link": "http://arxiv.org/abs/2512.09969v1",
    "authors": [
      "Paul Hueber",
      "Luca Peres",
      "Florian Pitters",
      "Alejandro Gloriani",
      "Oliver Rhodes"
    ],
    "categories": [
      "cs.CV",
      "cs.NE"
    ]
  },
  {
    "title": "Scalable Construction of Spiking Neural Networks using up to thousands of GPUs",
    "published": "2025-12-10T10:27:31Z",
    "updated": "2025-12-10T10:27:31Z",
    "summary": "Diverse scientific and engineering research areas deal with discrete, time-stamped changes in large systems of interacting delay differential equations. Simulating such complex systems at scale on high-performance computing clusters demands efficient management of communication and memory. Inspired by the human cerebral cortex -- a sparsely connected network of $\\mathcal{O}(10^{10})$ neurons, each forming $\\mathcal{O}(10^{3})$--$\\mathcal{O}(10^{4})$ synapses and communicating via short electrical pulses called spikes -- we study the simulation of large-scale spiking neural networks for computational neuroscience research. This work presents a novel network construction method for multi-GPU clusters and upcoming exascale supercomputers using the Message Passing Interface (MPI), where each process builds its local connectivity and prepares the data structures for efficient spike exchange across the cluster during state propagation. We demonstrate scaling performance of two cortical models using point-to-point and collective communication, respectively.",
    "link": "http://arxiv.org/abs/2512.09502v1",
    "authors": [
      "Bruno Golosio",
      "Gianmarco Tiddia",
      "JosÃ© Villamar",
      "Luca Pontisso",
      "Luca Sergi",
      "Francesco Simula",
      "Pooja Babu",
      "Elena Pastorelli",
      "Abigail Morrison",
      "Markus Diesmann",
      "Alessandro Lonardo",
      "Pier Stanislao Paolucci",
      "Johanna Senk"
    ],
    "categories": [
      "cs.DC",
      "cs.NE",
      "physics.comp-ph",
      "q-bio.NC"
    ]
  },
  {
    "title": "ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning",
    "published": "2025-12-11T18:59:46Z",
    "updated": "2025-12-11T18:59:46Z",
    "summary": "Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.",
    "link": "http://arxiv.org/abs/2512.10946v1",
    "authors": [
      "Wendi Chen",
      "Han Xue",
      "Yi Wang",
      "Fangyuan Zhou",
      "Jun Lv",
      "Yang Jin",
      "Shirun Tang",
      "Chuan Wen",
      "Cewu Lu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "title": "Any4D: Unified Feed-Forward Metric 4D Reconstruction",
    "published": "2025-12-11T18:57:39Z",
    "updated": "2025-12-11T18:57:39Z",
    "summary": "We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.",
    "link": "http://arxiv.org/abs/2512.10935v1",
    "authors": [
      "Jay Karhade",
      "Nikhil Keetha",
      "Yuchen Zhang",
      "Tanisha Gupta",
      "Akash Sharma",
      "Sebastian Scherer",
      "Deva Ramanan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ]
  },
  {
    "title": "Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit",
    "published": "2025-12-11T18:57:29Z",
    "updated": "2025-12-11T18:57:29Z",
    "summary": "Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.   The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.",
    "link": "http://arxiv.org/abs/2512.10934v1",
    "authors": [
      "Zamirddine Mari",
      "JÃ©rÃ´me Pasquet",
      "Julien Seinturier"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ]
  },
  {
    "title": "Decoupled Q-Chunking",
    "published": "2025-12-11T18:52:51Z",
    "updated": "2025-12-11T18:52:51Z",
    "summary": "Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences (\"chunks\") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.",
    "link": "http://arxiv.org/abs/2512.10926v1",
    "authors": [
      "Qiyang Li",
      "Seohong Park",
      "Sergey Levine"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ]
  },
  {
    "title": "Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation",
    "published": "2025-12-11T18:52:42Z",
    "updated": "2025-12-11T18:52:42Z",
    "summary": "Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.",
    "link": "http://arxiv.org/abs/2512.10925v1",
    "authors": [
      "Zamirddine Mari",
      "Mohamad Motasem Nawaf",
      "Pierre Drap"
    ],
    "categories": [
      "cs.LG",
      "cs.RO"
    ]
  },
  {
    "title": "Iterative Compositional Data Generation for Robot Control",
    "published": "2025-12-11T18:20:49Z",
    "updated": "2025-12-11T18:20:49Z",
    "summary": "Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.",
    "link": "http://arxiv.org/abs/2512.10891v1",
    "authors": [
      "Anh-Quan Pham",
      "Marcel Hussing",
      "Shubhankar P. Patankar",
      "Dani S. Bassett",
      "Jorge Mendez-Mendez",
      "Eric Eaton"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ]
  },
  {
    "title": "V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions",
    "published": "2025-12-11T17:14:37Z",
    "updated": "2025-12-11T17:14:37Z",
    "summary": "Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.",
    "link": "http://arxiv.org/abs/2512.10822v1",
    "authors": [
      "Mumuksh Tayal",
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya",
      "Ravi Prakash"
    ],
    "categories": [
      "cs.AI",
      "cs.RO"
    ]
  },
  {
    "title": "AERMANI-Diffusion: Regime-Conditioned Diffusion for Dynamics Learning in Aerial Manipulators",
    "published": "2025-12-11T16:10:32Z",
    "updated": "2025-12-11T16:10:32Z",
    "summary": "Aerial manipulators undergo rapid, configuration-dependent changes in inertial coupling forces and aerodynamic forces, making accurate dynamics modeling a core challenge for reliable control. Analytical models lose fidelity under these nonlinear and nonstationary effects, while standard data-driven methods such as deep neural networks and Gaussian processes cannot represent the diverse residual behaviors that arise across different operating conditions. We propose a regime-conditioned diffusion framework that models the full distribution of residual forces using a conditional diffusion process and a lightweight temporal encoder. The encoder extracts a compact summary of recent motion and configuration, enabling consistent residual predictions even through abrupt transitions or unseen payloads. When combined with an adaptive controller, the framework enables dynamics uncertainty compensation and yields markedly improved tracking accuracy in real-world tests.",
    "link": "http://arxiv.org/abs/2512.10773v1",
    "authors": [
      "Samaksh Ujjawal",
      "Shivansh Pratap Singh",
      "Naveen Sudheer Nair",
      "Rishabh Dev Yadav",
      "Wei Pan",
      "Spandan Roy"
    ],
    "categories": [
      "cs.RO"
    ]
  },
  {
    "title": "Distribution-Free Stochastic MPC for Joint-in-Time Chance-Constrained Linear Systems",
    "published": "2025-12-11T15:25:02Z",
    "updated": "2025-12-11T15:25:02Z",
    "summary": "This work presents a stochastic model predictive control (MPC) framework for linear systems subject to joint-in-time chance constraints under unknown disturbance distributions. Unlike existing stochastic MPC formulations that rely on parametric or Gaussian assumptions or require expensive offline computations, the proposed method leverages conformal prediction (CP) as a streamlined tool to construct finite-sample confidence regions for the system's stochastic error trajectories with minimal computational effort. These regions enable the relaxation of probabilistic constraints while providing formal guarantees. By employing an indirect feedback mechanism and a probabilistic set-based formulation, we prove recursive feasibility of the relaxed optimization problem and establish chance constraint satisfaction in closed-loop. Furthermore, we extend the approach to the more general output feedback setting with unknown measurement noise distributions. Given available noise samples, we establish satisfaction of the joint chance constraints and recursive feasibility via output measurements alone. Numerical examples demonstrate the effectiveness and advantages of the proposed method compared to existing approaches.",
    "link": "http://arxiv.org/abs/2512.10738v1",
    "authors": [
      "Lukas Vogel",
      "Andrea Carron",
      "Eleftherios E. Vlahakis",
      "Dimos V. Dimarogonas"
    ],
    "categories": [
      "eess.SY",
      "cs.RO"
    ]
  },
  {
    "title": "On the Stabilization of Rigid Formations on Regular Curves",
    "published": "2025-12-11T14:41:19Z",
    "updated": "2025-12-11T14:41:19Z",
    "summary": "This work deals with the problem of stabilizing a multi-agent rigid formation on a general class of planar curves. Namely, we seek to stabilize an equilateral polygonal formation on closed planar differentiable curves after a path sweep. The task of finding an inscribed regular polygon centered at the point of interest is solved via a randomized multi-start Newton-Like algorithm for which one is able to ascertain the existence of a minimizer. Then we design a continuous feedback law that guarantees convergence to, and sufficient sweeping of the curve, followed by convergence to the desired formation vertices while ensuring inter-agent avoidance. The proposed approach is validated through numerical simulations for different classes of curves and different rigid formations. Code: https://github.com/mebbaid/paper-elobaid-ifacwc-2026",
    "link": "http://arxiv.org/abs/2512.10700v1",
    "authors": [
      "Mohamed Elobaid",
      "Shinkyu Park",
      "Eric Feron"
    ],
    "categories": [
      "cs.RO"
    ]
  },
  {
    "title": "Designing Truthful Mechanisms for Asymptotic Fair Division",
    "published": "2025-12-11T18:22:27Z",
    "updated": "2025-12-11T18:22:27Z",
    "summary": "We study the problem of fairly allocating a set of $m$ goods among $n$ agents in the asymptotic setting, where each item's value for each agent is drawn from an underlying joint distribution. Prior works have shown that if this distribution is well-behaved, then an envy-free allocation exists with high probability when $m=Î©(n\\log{n})$ [Dickerson et al., 2014]. Under the stronger assumption that item values are independently and identically distributed (i.i.d.) across agents, this requirement improves to $m=Î©(n\\log{n}/\\log{\\log{n}})$, which is tight [Manurangsi and Suksompong, 2021]. However, these results rely on non-strategyproof mechanisms, such as maximum-welfare allocation or the round-robin algorithm, limiting their applicability in settings with strategic agents.   In this work, we extend the theory to a broader, more realistic class of joint value distributions, allowing for correlations among agents, atomicity, and unequal probabilities of having the highest value for an item. We show that envy-free allocations continue to exist with a high probability when $m=Î©(n\\log{n})$. More importantly, we give a new randomized mechanism that is truthful in expectation, efficiently implementable in polynomial time, and outputs envy-free allocations with high probability, answering an open question posed by [Manurangsi and Suksompong, 2017]. We further extend our mechanism to settings with asymptotic weighted fair division and multiple agent types and good types, proving new results in each case.",
    "link": "http://arxiv.org/abs/2512.10892v1",
    "authors": [
      "Jugal Garg",
      "Vishnu V. Narayan",
      "Yuang Eric Shen"
    ],
    "categories": [
      "cs.GT"
    ]
  },
  {
    "title": "Dynamics of multidimensional Simple Clock Auctions",
    "published": "2025-12-11T13:09:34Z",
    "updated": "2025-12-11T13:09:34Z",
    "summary": "Simple Clock Auctions (SCA) are a mechanism commonly used in spectrum auctions to sell lots of frequency bandwidths. We study such an auction with one player having access to perfect information against straightforward bidders. When the opponents' valuations satisfy the ordinary substitutes condition, we show that it is optimal to bid on a fixed lot overtime. In this setting, we consider a continuous-time version of the SCA auction in which the prices follow a differential inclusion with a piecewise-constant dynamics. We show that there exists a unique solution in the sense of Filippov. This guarantees that the continuous-time model coincides with the limit of the discrete-time auction when price increments tend to zero. Moreover, we show that the value function of this limit auction is piecewise linear (though possibly discontinuous). Finally, we illustrate these results by analyzing a simplified version of the multiband Australian spectrum auction of 2017.",
    "link": "http://arxiv.org/abs/2512.10614v1",
    "authors": [
      "Jad Zeroual",
      "Marianne Akian",
      "AurÃ©lien Bechler",
      "Matthieu Chardy",
      "StÃ©phane Gaubert"
    ],
    "categories": [
      "cs.GT",
      "math.CO",
      "math.OC"
    ]
  },
  {
    "title": "LLM-Auction: Generative Auction towards LLM-Native Advertising",
    "published": "2025-12-11T11:31:20Z",
    "updated": "2025-12-11T11:31:20Z",
    "summary": "The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties.",
    "link": "http://arxiv.org/abs/2512.10551v1",
    "authors": [
      "Chujie Zhao",
      "Qun Hu",
      "Shiping Song",
      "Dagui Chen",
      "Han Zhu",
      "Jian Xu",
      "Bo Zheng"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "title": "The $k$-flip Ising game",
    "published": "2025-12-11T07:51:36Z",
    "updated": "2025-12-11T07:51:36Z",
    "summary": "A partially parallel dynamical noisy binary choice (Ising) game in discrete time of $N$ players on complete graphs with $k$ players having a possibility of changing their strategies at each time moment called $k$-flip Ising game is considered. Analytical calculation of the transition matrix of game as well as the first two moments of the distribution of $\\varphi=N^+/N$, where $N^+$ is a number of players adhering to one of the two strategies, is presented. First two moments of the first hitting time distribution for sample trajectories corresponding to transition from a metastable and unstable states to a stable one are considered. A nontrivial dependence of these moments on $k$ for the decay of a metastable state is discussed. A presence of the minima at certain $k^*$ is attributed to a competition between $k$-dependent diffusion and restoring forces.",
    "link": "http://arxiv.org/abs/2512.10389v1",
    "authors": [
      "Kovalenko Aleksandr",
      "Andrey Leonidov"
    ],
    "categories": [
      "cs.GT",
      "cond-mat.stat-mech",
      "physics.soc-ph"
    ]
  },
  {
    "title": "Certifying Concavity and Monotonicity in Games via Sum-of-Squares Hierarchies",
    "published": "2025-12-11T05:20:10Z",
    "updated": "2025-12-11T05:20:10Z",
    "summary": "Concavity and its refinements underpin tractability in multiplayer games, where players independently choose actions to maximize their own payoffs which depend on other players' actions. In concave games, where players' strategy sets are compact and convex, and their payoffs are concave in their own actions, strong guarantees follow: Nash equilibria always exist and decentralized algorithms converge to equilibria. If the game is furthermore monotone, an even stronger guarantee holds: Nash equilibria are unique under strictness assumptions. Unfortunately, we show that certifying concavity or monotonicity is NP-hard, already for games where utilities are multivariate polynomials and compact, convex basic semialgebraic strategy sets -- an expressive class that captures extensive-form games with imperfect recall. On the positive side, we develop two hierarchies of sum-of-squares programs that certify concavity and monotonicity of a given game, and each level of the hierarchies can be solved in polynomial time. We show that almost all concave/monotone games are certified at some finite level of the hierarchies. Subsequently, we introduce SOS-concave/monotone games, which globally approximate concave/monotone games, and show that for any given game we can compute the closest SOS-concave/monotone game in polynomial time. Finally, we apply our techniques to canonical examples of imperfect recall extensive-form games.",
    "link": "http://arxiv.org/abs/2512.10292v1",
    "authors": [
      "Vincent Leon",
      "Iosif Sakos",
      "Ryann Sim",
      "Antonios Varvitsiotis"
    ],
    "categories": [
      "cs.GT",
      "cs.MA",
      "math.OC"
    ]
  },
  {
    "title": "Computing Evolutionarily Stable Strategies in Imperfect-Information Games",
    "published": "2025-12-11T04:38:55Z",
    "updated": "2025-12-11T04:38:55Z",
    "summary": "We present an algorithm for computing evolutionarily stable strategies (ESSs) in symmetric perfect-recall extensive-form games of imperfect information. Our main algorithm is for two-player games, and we describe how it can be extended to multiplayer games. The algorithm is sound and computes all ESSs in nondegenerate games and a subset of them in degenerate games which contain an infinite continuum of symmetric Nash equilibria. The algorithm is anytime and can be stopped early to find one or more ESSs. We experiment on an imperfect-information cancer signaling game as well as random games to demonstrate scalability.",
    "link": "http://arxiv.org/abs/2512.10279v1",
    "authors": [
      "Sam Ganzfried"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA",
      "econ.TH",
      "q-bio.PE"
    ]
  },
  {
    "title": "Does Timeboost Reduce MEV-Related Spam? Theory and Evidence from Layer-2 Transactions",
    "published": "2025-12-10T21:33:07Z",
    "updated": "2025-12-10T21:33:07Z",
    "summary": "Maximal extractable value opportunities often induce spam in Layer-2 blockchains: many identical transactions are submitted near simultaneously, most of which revert, wasting blockspace. We study Timeboost, a mechanism on Arbitrum that auctions a timestamp advantage, crucial under first-come first-served sequencing rules. We develop a game-theoretic model in which users choose the number of transaction copies to submit, and extend upon the baseline setting by modeling the Timeboost auction and subsequent transaction submission behavior. We show that Timeboost reduces spam and increases sequencer/DAO revenue in equilibrium relative to the baseline, transferring user payments from revert costs to auction bids. Empirically, we assemble mempool data from multiple Layer-2 networks, measuring spam via identical transactions submitted in narrow time intervals, and conduct an event study around Timeboost adoption on Arbitrum using other L2s as contemporaneous benchmarks. We find a decline in MEV-related spam and an increase in revenue on Arbitrum post-adoption, consistent with model predictions.",
    "link": "http://arxiv.org/abs/2512.10094v1",
    "authors": [
      "Brian Zhu"
    ],
    "categories": [
      "cs.GT"
    ]
  },
  {
    "title": "Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies",
    "published": "2025-12-10T14:29:04Z",
    "updated": "2025-12-10T14:29:04Z",
    "summary": "This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.",
    "link": "http://arxiv.org/abs/2512.09682v1",
    "authors": [
      "Mika Persson",
      "Jonas Lidman",
      "Jacob Ljungberg",
      "Samuel Sandelius",
      "Adam Andersson"
    ],
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ]
  },
  {
    "title": "Procurement Auctions with Predictions: Improved Frugality for Facility Location",
    "published": "2025-12-10T06:58:17Z",
    "updated": "2025-12-10T06:58:17Z",
    "summary": "We study the problem of designing procurement auctions for the strategic uncapacitated facility location problem: a company needs to procure a set of facility locations in order to serve its customers and each facility location is owned by a strategic agent. Each owner has a private cost for providing access to their facility (e.g., renting it or selling it to the company) and needs to be compensated accordingly. The goal is to design truthful auctions that decide which facilities the company should procure and how much to pay the corresponding owners, aiming to minimize the total cost, i.e., the monetary cost paid to the owners and the connection cost suffered by the customers (their distance to the nearest facility). We evaluate the performance of these auctions using the \\emph{frugality ratio}.   We first analyze the performance of the classic VCG auction in this context and prove that its frugality ratio is exactly $3$. We then leverage the learning-augmented framework and design auctions that are augmented with predictions regarding the owners' private costs. Specifically, we propose a family of learning-augmented auctions that achieve significant payment reductions when the predictions are accurate, leading to much better frugality ratios. At the same time, we demonstrate that these auctions remain robust even if the predictions are arbitrarily inaccurate, and maintain reasonable frugality ratios even under adversarially chosen predictions. We finally provide a family of ``error-tolerant'' auctions that maintain improved frugality ratios even if the predictions are only approximately accurate, and we provide upper bounds on their frugality ratio as a function of the prediction error.",
    "link": "http://arxiv.org/abs/2512.09367v1",
    "authors": [
      "Eric Balkanski",
      "Nicholas DeFilippis",
      "Vasilis Gkatzelis",
      "Xizhi Tan"
    ],
    "categories": [
      "cs.GT"
    ]
  },
  {
    "title": "The Illusion of Rationality: Tacit Bias and Strategic Dominance in Frontier LLM Negotiation Games",
    "published": "2025-12-10T02:17:28Z",
    "updated": "2025-12-10T02:17:28Z",
    "summary": "Large language models (LLMs) are increasingly being deployed as autonomous agents on behalf of institutions and individuals in economic, political, and social settings that involve negotiation. Yet this trend carries significant risks if their strategic behavior is not well understood. In this work, we revisit the NegotiationArena framework and run controlled simulation experiments on a diverse set of frontier LLMs across three multi turn bargaining games: Buyer Seller, Multi turn Ultimatum, and Resource Exchange. We ask whether improved general reasoning capabilities lead to rational, unbiased, and convergent negotiation strategies. Our results challenge this assumption. We find that models diverge into distinct, model specific strategic equilibria rather than converging to a unified optimal behavior. Moreover, strong numerical and semantic anchoring effects persist: initial offers are highly predictive of final agreements, and models consistently generate biased proposals by collapsing diverse internal valuations into rigid, generic price points. More concerningly, we observe dominance patterns in which some models systematically achieve higher payoffs than their counterparts. These findings underscore an urgent need to develop mechanisms to mitigate these issues before deploying such systems in real-world scenarios.",
    "link": "http://arxiv.org/abs/2512.09254v1",
    "authors": [
      "Manuel S. RÃ­os",
      "Ruben F. Manrique",
      "Nicanor Quijano",
      "Luis F. Giraldo"
    ],
    "categories": [
      "cs.GT",
      "cs.MA"
    ]
  },
  {
    "title": "Building Audio-Visual Digital Twins with Smartphones",
    "published": "2025-12-11T16:14:32Z",
    "updated": "2025-12-11T16:14:32Z",
    "summary": "Digital twins today are almost entirely visual, overlooking acoustics-a core component of spatial realism and interaction. We introduce AV-Twin, the first practical system that constructs editable audio-visual digital twins using only commodity smartphones. AV-Twin combines mobile RIR capture and a visual-assisted acoustic field model to efficiently reconstruct room acoustics. It further recovers per-surface material properties through differentiable acoustic rendering, enabling users to modify materials, geometry, and layout while automatically updating both audio and visuals. Together, these capabilities establish a practical path toward fully modifiable audio-visual digital twins for real-world environments.",
    "link": "http://arxiv.org/abs/2512.10778v1",
    "authors": [
      "Zitong Lan",
      "Yiwei Tang",
      "Yuhan Wang",
      "Haowen Lai",
      "Yiduo Hao",
      "Mingmin Zhao"
    ],
    "categories": [
      "cs.SD",
      "cs.MM",
      "eess.AS"
    ]
  },
  {
    "title": "Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering",
    "published": "2025-12-11T06:22:23Z",
    "updated": "2025-12-11T06:22:23Z",
    "summary": "Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.",
    "link": "http://arxiv.org/abs/2512.10327v1",
    "authors": [
      "Cai Xu",
      "Jinlong Liu",
      "Yilin Zhang",
      "Ziyu Guan",
      "Wei Zhao"
    ],
    "categories": [
      "cs.CV",
      "cs.MM"
    ]
  },
  {
    "title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models",
    "published": "2025-12-10T17:22:42Z",
    "updated": "2025-12-10T17:22:42Z",
    "summary": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.",
    "link": "http://arxiv.org/abs/2512.09841v1",
    "authors": [
      "Yijing Chen",
      "Yihan Wu",
      "Kaisi Guan",
      "Yuchen Ren",
      "Yuyue Wang",
      "Ruihua Song",
      "Liyun Ru"
    ],
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ]
  },
  {
    "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
    "published": "2025-12-10T16:57:31Z",
    "updated": "2025-12-10T16:57:31Z",
    "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
    "link": "http://arxiv.org/abs/2512.09824v1",
    "authors": [
      "Xianghao Kong",
      "Zeyu Zhang",
      "Yuwei Guo",
      "Zhuoran Zhao",
      "Songchun Zhang",
      "Anyi Rao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ]
  },
  {
    "title": "Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video",
    "published": "2025-12-10T05:51:59Z",
    "updated": "2025-12-11T04:18:41Z",
    "summary": "Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.",
    "link": "http://arxiv.org/abs/2512.09335v2",
    "authors": [
      "Seonghwa Choi",
      "Moonkyeong Choi",
      "Mingyu Jang",
      "Jaekyung Kim",
      "Jianfei Cai",
      "Wen-Huang Cheng",
      "Sanghoon Lee"
    ],
    "categories": [
      "cs.CV",
      "cs.MM"
    ]
  },
  {
    "title": "Gamification with Purpose: What Learners Prefer to Motivate Their Learning",
    "published": "2025-12-09T12:47:13Z",
    "updated": "2025-12-09T12:47:13Z",
    "summary": "This study investigates learners' preferences for game design elements (GDEs) in educational contexts to inform the development of purpose-driven gamification strategies. It emphasizes a learner-centered approach that aligns gamification design with pedagogical goals, while mitigating risks such as the erosion of intrinsic motivation. A systematic literature review was conducted to identify ten widely discussed GDEs. Visual prototypes representing each element were developed, and a best-worst scaling (BWS) survey with 125 participants was administered to elicit preference rankings. Qualitative feedback was also collected to uncover motivational drivers. Learners consistently preferred GDEs that support learning processes directly-most notably progress bars, concept maps, immediate feedback, and achievements. Qualitative analysis revealed six recurring motivational themes, including visible progress, content relevance, and constructive feedback. The findings suggest that learners value gamification elements that are meaningfully integrated with educational content and support intrinsic motivation. Purpose-aligned gamification should prioritize tools that visualize learning progress and provide actionable feedback, rather than relying solely on extrinsic incentives.",
    "link": "http://arxiv.org/abs/2512.08551v1",
    "authors": [
      "Kai Marquardt",
      "Mona Schulz",
      "Anne Koziolek",
      "Lucia Happe"
    ],
    "categories": [
      "cs.SE",
      "cs.CY",
      "cs.HC",
      "cs.MM"
    ]
  },
  {
    "title": "PAVAS: Physics-Aware Video-to-Audio Synthesis",
    "published": "2025-12-09T06:28:50Z",
    "updated": "2025-12-09T06:28:50Z",
    "summary": "Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.",
    "link": "http://arxiv.org/abs/2512.08282v1",
    "authors": [
      "Oh Hyun-Bin",
      "Yuhta Takida",
      "Toshimitsu Uesaka",
      "Tae-Hyun Oh",
      "Yuki Mitsufuji"
    ],
    "categories": [
      "cs.CV",
      "cs.MM",
      "cs.SD"
    ]
  },
  {
    "title": "A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification",
    "published": "2025-12-08T14:05:40Z",
    "updated": "2025-12-08T14:05:40Z",
    "summary": "This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).",
    "link": "http://arxiv.org/abs/2512.07571v1",
    "authors": [
      "Nicolas Calbucura",
      "Valentin Barriere"
    ],
    "categories": [
      "cs.CL",
      "cs.MM"
    ]
  },
  {
    "title": "Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits",
    "published": "2025-12-08T06:45:11Z",
    "updated": "2025-12-08T06:45:11Z",
    "summary": "We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.",
    "link": "http://arxiv.org/abs/2512.07209v1",
    "authors": [
      "Masato Ishii",
      "Akio Hayakawa",
      "Takashi Shibuya",
      "Yuki Mitsufuji"
    ],
    "categories": [
      "cs.MM",
      "cs.LG",
      "cs.SD"
    ]
  },
  {
    "title": "RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models",
    "published": "2025-12-07T12:04:46Z",
    "updated": "2025-12-07T12:04:46Z",
    "summary": "Pre-trained Vision-Language Models (VLMs), \\textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.",
    "link": "http://arxiv.org/abs/2512.06811v1",
    "authors": [
      "Xiang Lin",
      "Weixin Li",
      "Shu Guo",
      "Lihong Wang",
      "Di Huang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ]
  }
]